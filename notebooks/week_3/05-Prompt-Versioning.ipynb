{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991d51d2",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae9fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from jinja2 import Template\n",
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce62db9",
   "metadata": {},
   "source": [
    "#### RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(preprocessed_context, question):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructtions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- As an output you need to provide:\n",
    "\n",
    "* The answer to the question based on the provided context.\n",
    "* The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
    "* Short description (1-2 sentences) of the item based on the description provided in the context.\n",
    "\n",
    "- The short description should have the name of the item.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{preprocessed_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbbff6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_context = \"- a \\n- b\"\n",
    "question = \"What is a?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3064c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructtions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- As an output you need to provide:\n",
    "\n",
    "* The answer to the question based on the provided context.\n",
    "* The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
    "* Short description (1-2 sentences) of the item based on the description provided in the context.\n",
    "\n",
    "- The short description should have the name of the item.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{preprocessed_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79d99e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a shopping assistant that can answer questions about the products in stock.\n",
      "\n",
      "You will be given a question and a list of context.\n",
      "\n",
      "Instructtions:\n",
      "- You need to answer the question based on the provided context only.\n",
      "- Never use word context and refer to it as the available products.\n",
      "- As an output you need to provide:\n",
      "\n",
      "* The answer to the question based on the provided context.\n",
      "* The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
      "* Short description (1-2 sentences) of the item based on the description provided in the context.\n",
      "\n",
      "- The short description should have the name of the item.\n",
      "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
      "\n",
      "Context:\n",
      "- a \n",
      "- b\n",
      "\n",
      "Question:\n",
      "What is a?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de984f4",
   "metadata": {},
   "source": [
    "#### Jinja Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388ba212",
   "metadata": {},
   "outputs": [],
   "source": [
    "jinja_template = \"\"\"\n",
    "You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructtions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- As an output you need to provide:\n",
    "\n",
    "* The answer to the question based on the provided context.\n",
    "* The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
    "* Short description (1-2 sentences) of the item based on the description provided in the context.\n",
    "\n",
    "- The short description should have the name of the item.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{{ preprocessed_context }}\n",
    "\n",
    "Question:\n",
    "{{ question }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fb5be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(jinja_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda196b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_prompt = template.render(preprocessed_context=preprocessed_context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccc03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_jinja(preprocessed_context, question):\n",
    "\n",
    "    prompt = \"\"\"\n",
    "You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructtions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- As an output you need to provide:\n",
    "\n",
    "* The answer to the question based on the provided context.\n",
    "* The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
    "* Short description (1-2 sentences) of the item based on the description provided in the context.\n",
    "\n",
    "- The short description should have the name of the item.\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "\n",
    "Context:\n",
    "{{ preprocessed_context }}\n",
    "\n",
    "Question:\n",
    "{{ question }}\n",
    "\"\"\"\n",
    "\n",
    "    template = Template(prompt)\n",
    "    rendered_prompt = template.render(\n",
    "        preprocessed_context=preprocessed_context, \n",
    "        question=question\n",
    "    )\n",
    "\n",
    "    return rendered_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3372802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a shopping assistant that can answer questions about the products in stock.\n",
      "\n",
      "You will be given a question and a list of context.\n",
      "\n",
      "Instructtions:\n",
      "- You need to answer the question based on the provided context only.\n",
      "- Never use word context and refer to it as the available products.\n",
      "- As an output you need to provide:\n",
      "\n",
      "* The answer to the question based on the provided context.\n",
      "* The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
      "* Short description (1-2 sentences) of the item based on the description provided in the context.\n",
      "\n",
      "- The short description should have the name of the item.\n",
      "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
      "\n",
      "Context:\n",
      "- a \n",
      "- b\n",
      "\n",
      "Question:\n",
      "What is a?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(build_prompt_jinja(preprocessed_context, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d751578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template_config(yaml_file, prompt_key):\n",
    "\n",
    "    with open(yaml_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    template_content = config['prompts'][prompt_key]\n",
    "\n",
    "    template = Template(template_content)\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e844e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_jinja(preprocessed_context, question):\n",
    "\n",
    "    template = prompt_template_config(\"prompts/retrieval_generation.yaml\", \"retrieval_generation\")\n",
    "    \n",
    "    rendered_prompt = template.render(\n",
    "        preprocessed_context=preprocessed_context, \n",
    "        question=question\n",
    "    )\n",
    "\n",
    "    return rendered_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c63e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a shopping assistant that can answer questions about the products in stock.\n",
      "\n",
      "You will be given a question and a list of context.\n",
      "\n",
      "Instructtions:\n",
      "- You need to answer the question based on the provided context only.\n",
      "- Never use word context and refer to it as the available products.\n",
      "- As an output you need to provide:\n",
      "\n",
      "* The answer to the question based on the provided context.\n",
      "* The list of the IDs of the chunks that were used to answer the question. Only return the ones that are used in the answer.\n",
      "* Short description (1-2 sentences) of the item based on the description provided in the context.\n",
      "\n",
      "- The short description should have the name of the item.\n",
      "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
      "\n",
      "Context:\n",
      "{preprocessed_context}\n",
      "\n",
      "Question:\n",
      "{question}\n"
     ]
    }
   ],
   "source": [
    "print(build_prompt_jinja(preprocessed_context, question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271464a",
   "metadata": {},
   "source": [
    "#### Prompt Registries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc53b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10ff17cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithNotFoundError",
     "evalue": "Resource not found for /commits/-/retrieval-generation/latest. HTTPError('404 Client Error: Not Found for url: https://api.smith.langchain.com/commits/-/retrieval-generation/latest', '{\"error\":\"Commit not found\"}\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dushy\\Desktop\\globe\\study\\ai\\swirl25oct\\Project1\\ai-engineering-bootcamp-oct25\\venv\\Lib\\site-packages\\langsmith\\utils.py:159\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dushy\\Desktop\\globe\\study\\ai\\swirl25oct\\Project1\\ai-engineering-bootcamp-oct25\\venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://api.smith.langchain.com/commits/-/retrieval-generation/latest",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dushy\\Desktop\\globe\\study\\ai\\swirl25oct\\Project1\\ai-engineering-bootcamp-oct25\\venv\\Lib\\site-packages\\langsmith\\client.py:952\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    946\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.session.request(\n\u001b[32m    947\u001b[39m         method,\n\u001b[32m    948\u001b[39m         _construct_url(\u001b[38;5;28mself\u001b[39m.api_url, pathname),\n\u001b[32m    949\u001b[39m         stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    950\u001b[39m         **request_kwargs,\n\u001b[32m    951\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m \u001b[43mls_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dushy\\Desktop\\globe\\study\\ai\\swirl25oct\\Project1\\ai-engineering-bootcamp-oct25\\venv\\Lib\\site-packages\\langsmith\\utils.py:161\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m requests.HTTPError(\u001b[38;5;28mstr\u001b[39m(e), response.text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mHTTPError\u001b[39m: [Errno 404 Client Error: Not Found for url: https://api.smith.langchain.com/commits/-/retrieval-generation/latest] {\"error\":\"Commit not found\"}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLangSmithNotFoundError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ls_template = \u001b[43mls_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpull_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieval-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dushy\\Desktop\\globe\\study\\ai\\swirl25oct\\Project1\\ai-engineering-bootcamp-oct25\\venv\\Lib\\site-packages\\langsmith\\client.py:7740\u001b[39m, in \u001b[36mClient.pull_prompt\u001b[39m\u001b[34m(self, prompt_identifier, include_model)\u001b[39m\n\u001b[32m   7736\u001b[39m     \u001b[38;5;129m@contextlib\u001b[39m.contextmanager\n\u001b[32m   7737\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msuppress_langchain_beta_warning\u001b[39m():\n\u001b[32m   7738\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m7740\u001b[39m prompt_object = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpull_prompt_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_model\u001b[49m\n\u001b[32m   7742\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7743\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m suppress_langchain_beta_warning():\n\u001b[32m   7744\u001b[39m     prompt = loads(json.dumps(prompt_object.manifest))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dushy\\Desktop\\globe\\study\\ai\\swirl25oct\\Project1\\ai-engineering-bootcamp-oct25\\venv\\Lib\\site-packages\\langsmith\\client.py:7639\u001b[39m, in \u001b[36mClient.pull_prompt_commit\u001b[39m\u001b[34m(self, prompt_identifier, include_model)\u001b[39m\n\u001b[32m   7625\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Pull a prompt object from the LangSmith API.\u001b[39;00m\n\u001b[32m   7626\u001b[39m \n\u001b[32m   7627\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   7634\u001b[39m \u001b[33;03m    ValueError: If no commits are found for the prompt.\u001b[39;00m\n\u001b[32m   7635\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   7636\u001b[39m owner, prompt_name, commit_hash = ls_utils.parse_prompt_identifier(\n\u001b[32m   7637\u001b[39m     prompt_identifier\n\u001b[32m   7638\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m7639\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7640\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   7641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7642\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/commits/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mowner\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprompt_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   7643\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m?include_model=true\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43minclude_model\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   7644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7645\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas.PromptCommit(\n\u001b[32m   7647\u001b[39m     **{\u001b[33m\"\u001b[39m\u001b[33mowner\u001b[39m\u001b[33m\"\u001b[39m: owner, \u001b[33m\"\u001b[39m\u001b[33mrepo\u001b[39m\u001b[33m\"\u001b[39m: prompt_name, **response.json()}\n\u001b[32m   7648\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dushy\\Desktop\\globe\\study\\ai\\swirl25oct\\Project1\\ai-engineering-bootcamp-oct25\\venv\\Lib\\site-packages\\langsmith\\client.py:992\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithAuthError(\n\u001b[32m    988\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAuthentication failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    989\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    990\u001b[39m     )\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m404\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithNotFoundError(\n\u001b[32m    993\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    995\u001b[39m     )\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithConflictError(\n\u001b[32m    998\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    999\u001b[39m     )\n",
      "\u001b[31mLangSmithNotFoundError\u001b[39m: Resource not found for /commits/-/retrieval-generation/latest. HTTPError('404 Client Error: Not Found for url: https://api.smith.langchain.com/commits/-/retrieval-generation/latest', '{\"error\":\"Commit not found\"}\\n')"
     ]
    }
   ],
   "source": [
    "ls_template = ls_client.pull_prompt(\"retrieval-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7cc42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
